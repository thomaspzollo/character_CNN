{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_cnn_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSXuAfk7blof"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95WK0VRcLEWu"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import string\n",
        "import random\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import seaborn as sns\n",
        "import statistics\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import namedtuple\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upOx1ulULKY1"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYzh3L5vLQyN"
      },
      "source": [
        "sys.path.insert(0,\"/content/drive/My Drive/ML_proj/helpers\")\n",
        "import embedding_utils as e_utils\n",
        "import lyric_utils as l_utils\n",
        "import nn_utils as nn_utils\n",
        "import cnn_utils as cnn_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvUz64OJLTvi"
      },
      "source": [
        "## Large (14 Artist) Dataset\n",
        "# artists = ['Kendrick Lamar', 'Eminem', 'DaBaby', 'RiFF RAFF', 'Lil Baby', 'Travis Scott', 'Jadakiss', 'Tory Lanez', 'Kanye West', 'Post Malone', 'Drake', 'Nicki Minaj', 'Cardi B', 'The Weeknd']\n",
        "\n",
        "## Small (8 Artist) Dataset\n",
        "artists = ['Eminem', 'DaBaby', 'RiFF RAFF', 'Lil Baby', 'Travis Scott', 'Drake', 'Nicki Minaj', 'The Weeknd']\n",
        "\n",
        "art2idx = {}\n",
        "idx2art = {}\n",
        "n_artists = 0\n",
        "\n",
        "for artist in artists:\n",
        "\n",
        "  art2idx[artist] = n_artists\n",
        "  idx2art[n_artists] = artist\n",
        "  n_artists += 1\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/My Drive/ML_proj/train_df_large_long1.csv')\n",
        "val_df = pd.read_csv('/content/drive/My Drive/ML_proj/val_df_large_long1.csv')\n",
        "test_df = pd.read_csv('/content/drive/My Drive/ML_proj/test_df_large_long1.csv')\n",
        "\n",
        "train_df = train_df[train_df['Artist'].isin(artists)]\n",
        "val_df = val_df[val_df['Artist'].isin(artists)]\n",
        "test_df = test_df[test_df['Artist'].isin(artists)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcPdjiBsLYYv"
      },
      "source": [
        "print(str(n_artists) + \" artists\")\n",
        "print(train_df)\n",
        "print(val_df)\n",
        "print(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydUXyUwjMs11"
      },
      "source": [
        "def words2charindices(sents):\n",
        "    \"\"\" Convert list of sentences of words into list of list of list of character indices.\n",
        "    @param sents (list[list[str]]): sentence(s) in words\n",
        "    @return word_ids (list[list[list[int]]]): sentence(s) in indices\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for s in sents:\n",
        "        y = []\n",
        "        for w in s:\n",
        "            x = [1]\n",
        "            for c in list(w):\n",
        "                if c in char2id:\n",
        "                    x.append(char2id[c])\n",
        "                else:\n",
        "                    x.append(char2id['<unk>'])\n",
        "            x.append(2)\n",
        "            y.append(x)\n",
        "        results.append(y)\n",
        "\n",
        "    return results\n",
        "\n",
        "def to_input_tensor_char(sents, device,max_sent_len):\n",
        "    \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
        "    shorter sentences.\n",
        "\n",
        "    @param sents (List[List[str]]): list of sentences (words)\n",
        "    @param device: device on which to load the tensor, i.e. CPU or GPU\n",
        "\n",
        "    @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n",
        "    \"\"\"\n",
        "    char_ids = words2charindices(sents)\n",
        "    chars_t = pad_sents_char(char_ids, char2id['<pad>'],max_sent_len)\n",
        "    chars_var = torch.tensor(chars_t, dtype=torch.long,device=device)\n",
        "    chars_var = chars_var.permute(1,0,2)\n",
        "    chars_var = chars_var.contiguous()\n",
        "\n",
        "    return chars_var\n",
        "    \n",
        "\n",
        "def pad_sents_char(sents, char_pad_token,max_sent_length=150):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch and max_word_length.\n",
        "    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()`\n",
        "        from `vocab.py`\n",
        "    @param char_pad_token (int): index of the character-padding token\n",
        "    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter\n",
        "        than the max length sentence/word are padded out with the appropriate pad token, such that\n",
        "        each sentence in the batch now has same number of words and each word has an equal\n",
        "        number of characters\n",
        "        Output shape: (batch_size, max_sentence_length, max_word_length)\n",
        "    \"\"\"\n",
        "    max_word_length = 21\n",
        "    sents_padded = []\n",
        "\n",
        "    max_len = max(len(s) for s in sents)\n",
        "    batch_size = len(sents)\n",
        "\n",
        "    for s in sents:\n",
        "        s_padded = []\n",
        "        for w in s:\n",
        "            w_padded = []\n",
        "            for c in w:\n",
        "                w_padded.append(c)\n",
        "                if len(w_padded) >= max_word_length:\n",
        "                    break\n",
        "            w_diff = max_word_length - len(w_padded)\n",
        "            for i in range(w_diff):\n",
        "                w_padded.append(char_pad_token)\n",
        "            s_padded.append(w_padded)\n",
        "            if len(s_padded) >= max_sent_length:\n",
        "                break\n",
        "        s_diff = max_sent_length - len(s_padded)\n",
        "        for i in range(s_diff):\n",
        "            s_padded.append( max_word_length*[char_pad_token] )\n",
        "        sents_padded.append(s_padded)\n",
        "    return sents_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONWU2bArUUxM"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self, char_embed_size,word_embed_size, kernel_size=5,padding=1):\n",
        "\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.pool = nn.MaxPool1d(3)\n",
        "    self.conv_layer1 = nn.Conv1d(char_embed_size, word_embed_size, kernel_size, padding)\n",
        "    self.conv_layer2 = nn.Conv1d(word_embed_size, word_embed_size, kernel_size, padding)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self, t):\n",
        "\n",
        "    # xconv = self.conv_layer(t)\n",
        "    # xconvout = torch.max(self.relu(xconv), dim=2)[0]\n",
        "\n",
        "    xconv = self.conv_layer1(t)\n",
        "    xconv = self.relu(xconv)\n",
        "    xconv = self.pool(xconv)\n",
        "\n",
        "    xconv = self.conv_layer2(xconv)\n",
        "    xconvout = torch.max(self.relu(xconv), dim=2)[0]\n",
        "\n",
        "    return xconvout\n",
        "\n",
        "class Highway(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_size):\n",
        "\n",
        "    super(Highway, self).__init__()\n",
        "\n",
        "    self.proj_layer = nn.Linear(embed_size, embed_size, bias=True)\n",
        "    self.gate_layer = nn.Linear(embed_size, embed_size, bias=True)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, t):\n",
        "\n",
        "    xproj = self.relu( self.proj_layer(t) )\n",
        "    xgate = self.sigmoid( self.gate_layer(t) )\n",
        "\n",
        "    xhighway = (xgate * xproj) + ((1 - xgate) * t)\n",
        "\n",
        "    return xhighway"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqP6uDb8PgKR"
      },
      "source": [
        "class ModelEmbeddings(nn.Module): \n",
        "  \"\"\"\n",
        "  Class that converts input words to their CNN-based embeddings.\n",
        "  \"\"\"\n",
        "  def __init__(self, embed_size, vocab):\n",
        "    \"\"\"\n",
        "    Init the Embedding layer for one language\n",
        "    @param embed_size (int): Embedding size (dimensionality) for the output \n",
        "    @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n",
        "    \"\"\"\n",
        "    super(ModelEmbeddings, self).__init__()\n",
        "\n",
        "    self.vocab = vocab\n",
        "    self.size_of_vocab = len(self.vocab['char2id'])\n",
        "    pad_token_idx = vocab['char2id']['<pad>']\n",
        "\n",
        "    self.embed_size = embed_size\n",
        "    self.char_embed_size = embed_size\n",
        "    self.kernel_size = 3\n",
        "\n",
        "    self.char_embeddings = nn.Embedding(self.size_of_vocab, self.char_embed_size,padding_idx=pad_token_idx)\n",
        "\n",
        "    self.highway = Highway(self.embed_size)\n",
        "    self.cnn = CNN(self.char_embed_size, self.embed_size, self.kernel_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    \"\"\"\n",
        "    Looks up character-based CNN embeddings for the words in a batch of sentences.\n",
        "    @param input_tensor: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n",
        "        each integer is an index into the character vocabulary\n",
        "\n",
        "    @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the \n",
        "        CNN-based embeddings for each word of the sentences in the batch\n",
        "    \"\"\"\n",
        "    x_pad = input_tensor\n",
        "    sent_length, batch_size, max_word_length = x_pad.shape\n",
        "    x_emb = self.char_embeddings(input_tensor)\n",
        "\n",
        "    x_resh = x_emb.permute(0,1,3,2)\n",
        "    x_resh = x_resh.view(-1, self.char_embed_size, max_word_length)\n",
        "\n",
        "    x_conv_out = self.cnn(x_resh)\n",
        "    x_highway = self.highway(x_conv_out)        \n",
        "    x_highway = x_highway.view(sent_length, batch_size, self.embed_size)\n",
        "\n",
        "    x_word_embed = self.dropout(x_highway)\n",
        "    return x_word_embed\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-0cCVAS77W"
      },
      "source": [
        "stopwords = []\n",
        "lyrics = train_df['Lyric'].tolist() + val_df['Lyric'].tolist() + test_df['Lyric'].tolist()\n",
        "# lyrics = [l_utils.string_process(str(lyric),stopwords,False) for lyric in lyrics]\n",
        "# lyrics = [\" \".join(lyric) for lyric in lyrics]\n",
        "\n",
        "idx_lyrics = [(j,lyric) for j,lyric in enumerate(lyrics)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8lIwtNBLZ_V"
      },
      "source": [
        "X_train = idx_lyrics[:train_df.shape[0]]\n",
        "X_val = idx_lyrics[train_df.shape[0]:train_df.shape[0]+val_df.shape[0]]\n",
        "X_test = idx_lyrics[train_df.shape[0]:train_df.shape[0]+val_df.shape[0]]\n",
        "\n",
        "y_train = train_df['Artist'].to_list()\n",
        "y_train = [art2idx[x] for x in y_train]\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "y_val = val_df['Artist'].to_list()\n",
        "y_val = [art2idx[x] for x in y_val]\n",
        "y_val = np.asarray(y_val)\n",
        "\n",
        "y_test = test_df['Artist'].to_list()\n",
        "y_test = [art2idx[x] for x in y_test]\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "print(len(X_val))\n",
        "print(len(y_val))\n",
        "print(len(X_test))\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjOqK1xygHY0"
      },
      "source": [
        "# #BUILD SBERT EMBEDS\n",
        "# print('Building SBert Embeds')\n",
        "\n",
        "# sbert = SentenceTransformer('stsb-roberta-base')\n",
        "# l_text = list(map(l_utils.myMap, lyrics))\n",
        "# l_embeds = sbert.encode(l_text)\n",
        "# l_embeds = np.array(l_embeds)\n",
        "# l_embeds = torch.from_numpy(l_embeds)\n",
        "# print(l_embeds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fj8oKiBRm1n"
      },
      "source": [
        "# # BUILD BoW EMBEDS\n",
        "# print('Building BoW Embeds')\n",
        "\n",
        "# doc_embed_size = 768\n",
        "# stopwords = []\n",
        "# n_train = train_df.shape[0]\n",
        "\n",
        "# corpus = []\n",
        "\n",
        "# for i,lyric in enumerate(lyrics):\n",
        "#   # lyric = str(line[1])\n",
        "#   proc = l_utils.string_process(lyric,stopwords,False)\n",
        "#   corpus.append( proc )\n",
        "\n",
        "# M_embeddings, word2Ind = e_utils.compute_embedding_matrix(corpus)\n",
        "# print(M_embeddings.shape)\n",
        "\n",
        "# l_embeds = e_utils.reduce_to_k_dim(M_embeddings, doc_embed_size,n_train)\n",
        "# l_embeds = torch.Tensor(l_embeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTrUsh0IR4iH"
      },
      "source": [
        "# ##BUILD doc2vec EMBEDS\n",
        "# print('Building doc2vec Embeds')\n",
        "\n",
        "# def tagged_document(list_of_list_of_words,targets=None):\n",
        "#   for i, list_of_words in enumerate(list_of_list_of_words):\n",
        "#     if targets is not None:\n",
        "#       yield gensim.models.doc2vec.TaggedDocument(list_of_words, [int(targets[i])])\n",
        "#     else:\n",
        "#       yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
        "\n",
        "# stopwords = []\n",
        "# data =  list(train_df['Lyric'].items())\n",
        "# data = [l_utils.string_process(str(lyric[1]),stopwords,False) for lyric in data]\n",
        "\n",
        "# targets = y_train\n",
        "# data_for_training = list(tagged_document(data,targets))\n",
        "\n",
        "# model = gensim.models.doc2vec.Doc2Vec(vector_size=768, min_count=0, epochs=10)\n",
        "# model.build_vocab(data_for_training)\n",
        "# model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# lines = [l_utils.string_process(str(lyric),stopwords,False) for lyric in lyrics]\n",
        "\n",
        "# embeddings = []\n",
        "# for i, line in enumerate(lines):\n",
        "  \n",
        "#   vec = model.infer_vector(lines[i])\n",
        "#   embeddings.append(vec)\n",
        "\n",
        "# l_embeds = np.array(embeddings)\n",
        "# l_embeds = torch.Tensor(l_embeds)\n",
        "# print(l_embeds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-MtpsZuL-AV"
      },
      "source": [
        "char_list = list(\"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZ ’—‘abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]\"\"\")\n",
        "\n",
        "char2id = dict() # Converts characters to integers\n",
        "char2id['<pad>'] = 0\n",
        "char2id['{'] = 1\n",
        "char2id['}'] = 2\n",
        "char2id['<unk>'] = 3\n",
        "for i, c in enumerate(char_list):\n",
        "    char2id[c] = len(char2id)\n",
        "char_unk = char2id['<unk>']\n",
        "start_of_word = char2id[\"{\"]\n",
        "end_of_word = char2id[\"}\"]\n",
        "assert start_of_word+1 == end_of_word\n",
        "\n",
        "id2char = {v: k for k, v in char2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxihEb_hWozP"
      },
      "source": [
        "class TomNet(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, hidden_size, vocab, lookup, num_class, device, use_cnn = True, max_sent_len = 150, dropout_rate=0.2):\n",
        "        \"\"\" Init TomNet Model.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param hidden_size (int): Hidden Size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        @param dropout_rate (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "        super(TomNet, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "\n",
        "        self.max_sent_len = max_sent_len\n",
        "        self.use_cnn = use_cnn\n",
        "        \n",
        "        self.model_embeddings_source = ModelEmbeddings(embed_size, vocab)\n",
        "        self.text_embeddings = nn.Embedding.from_pretrained(lookup)\n",
        "\n",
        "        self.word_embed_size = embed_size\n",
        "\n",
        "        if self.use_cnn:\n",
        "            self.num_features = 768+self.hidden_size\n",
        "        else:\n",
        "            self.num_features = 768\n",
        "        self.num_class = num_class\n",
        "\n",
        "        self.n1 = 1024\n",
        "        self.n2 = 1024\n",
        "        self.n3 = 512\n",
        "\n",
        "        proj_size = self.word_embed_size*self.max_sent_len\n",
        "        self.cnn_proj = nn.Linear(proj_size, self.hidden_size)\n",
        "        self.layer_1 = nn.Linear(self.num_features, self.n1)\n",
        "        self.layer_2 = nn.Linear(self.n1, self.n2)\n",
        "        self.layer_3 = nn.Linear(self.n2, self.n3)\n",
        "        self.layer_out = nn.Linear(self.n3, self.num_class)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "        self.batchnormp = nn.BatchNorm1d(self.hidden_size)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(self.n1)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(self.n2)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(self.n3)\n",
        "\n",
        "    def forward(self, source):\n",
        "        \"\"\" Take a mini-batch of source sentences, compute scores\n",
        "         under the language models learned by the TomNet system.\n",
        "\n",
        "        @param source (List[List[str]]): list of source sentence tokens\n",
        "\n",
        "        @returns scores (Tensor): a variable/tensor of shape (b, ). Here b = batch size.\n",
        "        \"\"\"\n",
        "        text = []\n",
        "        text_ids = []\n",
        "        for s in source:\n",
        "            text_ids.append(s[0])\n",
        "            text.append(s[1])\n",
        "        if self.use_cnn:\n",
        "            \n",
        "            source_padded_chars = to_input_tensor_char(text,self.device,self.max_sent_len)\n",
        "            X = self.model_embeddings_source(source_padded_chars)\n",
        "\n",
        "            X = X.reshape(-1, X.shape[1]).permute(1,0)\n",
        "            X = self.cnn_proj(X)\n",
        "            X = self.batchnormp(X)\n",
        "            X = self.relu(X)\n",
        "        \n",
        "            input = torch.tensor(text_ids, dtype=torch.long, device=device)\n",
        "            sent_embeds = self.text_embeddings(input)\n",
        "\n",
        "            x = torch.cat((X,sent_embeds),1)\n",
        "\n",
        "        else:\n",
        "            input = torch.tensor(text_ids, dtype=torch.long, device=device)\n",
        "            x = self.text_embeddings(input)\n",
        "\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW3D87I0Te5-"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_CLASSES = n_artists\n",
        "\n",
        "vocab = {}\n",
        "vocab['char2id'] = char2id\n",
        "vocab['id2char'] = id2char\n",
        "\n",
        "word_embed_size = e_char = 50\n",
        "hidden_size = 512\n",
        "use_cnn = False\n",
        "max_sent_len = 120\n",
        "\n",
        "nn_model = TomNet(word_embed_size,hidden_size,vocab,l_embeds,n_artists,device,use_cnn,max_sent_len)\n",
        "nn_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=LEARNING_RATE)\n",
        "print(nn_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqn7fcJmvdTd"
      },
      "source": [
        "accuracy_stats = {'train': [],\"val\": []}\n",
        "loss_stats = {'train': [], \"val\": []}\n",
        "\n",
        "train_data = list(zip(X_train, y_train))\n",
        "val_data = list(zip(X_val, y_val))\n",
        "\n",
        "fp = \"/content/drive/My Drive/ML_proj/saved_models/lyric_small_cnn_200\"\n",
        "\n",
        "best_val_acc = 0.0\n",
        "print(\"Begin training TomNet.\")\n",
        "for e in tqdm(range(1, EPOCHS+1)):\n",
        "    \n",
        "  # TRAINING\n",
        "  nn_model.train()\n",
        "  \n",
        "  train_epoch_loss = 0\n",
        "  train_epoch_acc = 0\n",
        "  count_train = 0\n",
        "  for X_train_batch, y_train_batch in nn_utils.batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n",
        "\n",
        "    y_train_batch = torch.tensor(y_train_batch, dtype=torch.long).to(device)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    y_train_pred = nn_model(X_train_batch)\n",
        "\n",
        "    train_loss = criterion(y_train_pred, y_train_batch)\n",
        "    train_acc = nn_utils.multi_acc(y_train_pred, y_train_batch)\n",
        "    \n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_epoch_loss += train_loss.item()\n",
        "    train_epoch_acc += train_acc.item()\n",
        "    count_train += 1\n",
        "\n",
        "  # VALIDATION\n",
        "  with torch.no_grad():\n",
        "        \n",
        "    nn_model.eval()\n",
        "\n",
        "    val_epoch_loss = 0\n",
        "    val_epoch_acc = 0\n",
        "    count_val = 0\n",
        "    for X_val_batch, y_val_batch in val_data:\n",
        "\n",
        "      y_val_pred = nn_model([X_val_batch])\n",
        "      y_val_batch = torch.tensor([y_val_batch]).to(device)         \n",
        "      val_loss = criterion(y_val_pred, y_val_batch)\n",
        "      val_acc = nn_utils.multi_acc(y_val_pred, y_val_batch)\n",
        "      \n",
        "      val_epoch_loss += val_loss.item()\n",
        "      val_epoch_acc += val_acc.item()\n",
        "      count_val += 1\n",
        "            \n",
        "  loss_stats['train'].append(train_epoch_loss/count_train)\n",
        "  loss_stats['val'].append(val_epoch_loss/count_val)\n",
        "  accuracy_stats['train'].append(train_epoch_acc/count_train)\n",
        "  accuracy_stats['val'].append(val_epoch_acc/count_val)\n",
        "\n",
        "  val_acc = val_epoch_acc/count_val\n",
        "  if val_acc > best_val_acc and e > 20:\n",
        "    best_val_acc = val_acc\n",
        "    torch.save(nn_model.state_dict(), fp+\"_best\")\n",
        "                            \n",
        "  print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/count_train:.5f} | Train Acc: {train_epoch_acc/count_train:.3f} | Val Loss: {val_epoch_loss/count_val:.5f} | Val Acc: {val_epoch_acc/count_val:.3f}')\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSRPOl_A0K_x"
      },
      "source": [
        "# Save model:\n",
        "# filepath = \"/content/drive/My Drive/ML_proj/saved_models/lyric_small_cnn_200\"\n",
        "filepath = fp\n",
        "torch.save(nn_model.state_dict(), filepath)\n",
        "\n",
        "# Later to restore:\n",
        "# filepath = fp+\"_best\"\n",
        "# nn_model.load_state_dict(torch.load(filepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzTy7sWwJK0L"
      },
      "source": [
        "test_data = list(zip(X_test, y_test))\n",
        "\n",
        "y_pred_list = []\n",
        "y_probs_list = []\n",
        "found = 0\n",
        "top_n = 3\n",
        "\n",
        "with torch.no_grad():\n",
        "    nn_model.eval()\n",
        "    for X_batch, y_batch in test_data:\n",
        "        y_test_pred = nn_model([X_batch])\n",
        "\n",
        "        probs = y_test_pred.tolist()[0]\n",
        "        sm = torch.softmax(torch.tensor(probs),0).tolist()\n",
        "        y_probs_list.append(sm)\n",
        "        \n",
        "        s = list( zip( probs, art2idx.keys() ) )\n",
        "        \n",
        "        s.sort(reverse=True)\n",
        "        top = s[0:top_n]\n",
        "        guesses = [i[1] for i in top]\n",
        "        ans = idx2art[y_batch]\n",
        "\n",
        "        if ans in guesses:\n",
        "            found += 1\n",
        "        \n",
        "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n",
        "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "top_n_sc = found/len(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_onmtP2JzBy"
      },
      "source": [
        "print('n_artists = ' + str(n_artists) + '\\n')\n",
        "print('Test Results')\n",
        "print('-------------')\n",
        "\n",
        "score = accuracy_score(y_pred_list,y_test)\n",
        "print('Accuracy: ' + str(score))\n",
        "\n",
        "print('ROC AUC: ' + str(roc_auc_score(y_test, y_probs_list, multi_class='ovr')))\n",
        "\n",
        "f1 = f1_score(y_test, y_pred_list, average='macro')\n",
        "print('Avg f1: ' + str(f1))\n",
        "\n",
        "print('Top ' + str(top_n) + \" score: \" + str(top_n_sc))\n",
        "\n",
        "print('Metrics:')\n",
        "test_met = metrics.classification_report(y_test, y_pred_list, target_names=artists, digits=3)\n",
        "print(test_met)\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred_list))\n",
        "x_axis_labels = y_axis_labels = list(art2idx.keys())\n",
        "sns.heatmap(confusion_matrix_df, annot=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU9Vvqd9SYxS"
      },
      "source": [
        "# Create dataframes\n",
        "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "# Plot the dataframes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10,7))\n",
        "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes).set_title('Train-Val Accuracy/Epoch')\n",
        "# sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCaU9MGAH5nc"
      },
      "source": [
        "import netron\n",
        "\n",
        "for X_train_batch, y_train_batch in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n",
        "  y_train_batch = torch.tensor(y_train_batch, dtype=torch.long).to(device)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  y_train_pred = nn_model(X_train_batch)\n",
        "  break\n",
        "\n",
        "input_names = ['Document Embedding (Batch)']\n",
        "output_names = ['Class Probabilities']\n",
        "torch.onnx.export(nn_model, X_train_batch, 'cnn_v2.onnx', input_names=input_names, output_names=output_names)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}